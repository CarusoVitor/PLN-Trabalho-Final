{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import torch\n",
    "import datasets\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from typing import Union\n",
    "from skmultilearn.model_selection import iterative_train_test_split\n",
    "\n",
    "MODEL_NAME = \"neuralmind/bert-base-portuguese-cased\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/victor/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 285,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2226 entries, 0 to 2225\n",
      "Data columns (total 8 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   id        2226 non-null   object\n",
      " 1   text      2226 non-null   object\n",
      " 2   Anger     2226 non-null   int64 \n",
      " 3   Disgust   2226 non-null   int64 \n",
      " 4   Fear      2226 non-null   int64 \n",
      " 5   Joy       2226 non-null   int64 \n",
      " 6   Sadness   2226 non-null   int64 \n",
      " 7   Surprise  2226 non-null   int64 \n",
      "dtypes: int64(6), object(2)\n",
      "memory usage: 139.3+ KB\n"
     ]
    }
   ],
   "source": [
    "file = 'public_data/train/track_a/ptbr.csv'\n",
    "data = pd.read_csv(file)\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>Anger</th>\n",
       "      <th>Disgust</th>\n",
       "      <th>Fear</th>\n",
       "      <th>Joy</th>\n",
       "      <th>Sadness</th>\n",
       "      <th>Surprise</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ptbr_train_track_a_00001</td>\n",
       "      <td>minha vó me disse que era frango e eu comi, ti...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ptbr_train_track_a_00002</td>\n",
       "      <td>Está e a nossa deputada Benedita linda guerrei...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ptbr_train_track_a_00003</td>\n",
       "      <td>só falta as roupas kkkkkkkkkkk</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ptbr_train_track_a_00004</td>\n",
       "      <td>Eu tmb. Comecei a sair de casa agora (fui pela...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ptbr_train_track_a_00005</td>\n",
       "      <td>Peço a Deus que nossos dirigentes tenham realm...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         id  \\\n",
       "0  ptbr_train_track_a_00001   \n",
       "1  ptbr_train_track_a_00002   \n",
       "2  ptbr_train_track_a_00003   \n",
       "3  ptbr_train_track_a_00004   \n",
       "4  ptbr_train_track_a_00005   \n",
       "\n",
       "                                                text  Anger  Disgust  Fear  \\\n",
       "0  minha vó me disse que era frango e eu comi, ti...      0        0     0   \n",
       "1  Está e a nossa deputada Benedita linda guerrei...      0        0     0   \n",
       "2                     só falta as roupas kkkkkkkkkkk      0        0     0   \n",
       "3  Eu tmb. Comecei a sair de casa agora (fui pela...      0        0     0   \n",
       "4  Peço a Deus que nossos dirigentes tenham realm...      0        0     0   \n",
       "\n",
       "   Joy  Sadness  Surprise  \n",
       "0    0        1         0  \n",
       "1    1        0         0  \n",
       "2    1        0         0  \n",
       "3    0        1         0  \n",
       "4    0        0         0  "
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pré-processamento dos dados\n",
    "1. Case folding\n",
    "2. Remover stop words\n",
    "3. Remover acentuação e pontuação\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a',\n",
       " 'à',\n",
       " 'ao',\n",
       " 'aos',\n",
       " 'aquela',\n",
       " 'aquelas',\n",
       " 'aquele',\n",
       " 'aqueles',\n",
       " 'aquilo',\n",
       " 'as',\n",
       " 'às',\n",
       " 'até',\n",
       " 'com',\n",
       " 'como',\n",
       " 'da',\n",
       " 'das',\n",
       " 'de',\n",
       " 'dela',\n",
       " 'delas',\n",
       " 'dele']"
      ]
     },
     "execution_count": 288,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('portuguese')\n",
    "stopwords[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"clean_text\"] = data[\"text\"].copy()\n",
    "# Case folding\n",
    "data[\"clean_text\"] = data[\"clean_text\"].str.lower()\n",
    "# Remover stopwords\n",
    "data[\"clean_text\"] = data[\"clean_text\"].replace({r\"\\b\" + stopword + r\"\\b\": \"\" for stopword in stopwords}, regex=True)\n",
    "# Remover acentuação e pontuação\n",
    "data[\"clean_text\"] = data[\"clean_text\"].str.replace(r\"[\\.!?\\\\\\-.,]\", \"\", regex=True)\n",
    "data[\"clean_text\"] = data[\"clean_text\"].str.replace(r\"\\s+\", \" \", regex=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0       minha vó me disse que era frango e eu comi, ti...\n",
       " 1       Está e a nossa deputada Benedita linda guerrei...\n",
       " 2                          só falta as roupas kkkkkkkkkkk\n",
       " 3       Eu tmb. Comecei a sair de casa agora (fui pela...\n",
       " 4       Peço a Deus que nossos dirigentes tenham realm...\n",
       "                               ...                        \n",
       " 2221              Eu acho que o CAP vai surpreender hein.\n",
       " 2222    23:59 - Lula sabia de toda a corrupção no seu ...\n",
       " 2223    O Brasil precisa URGENTE de pessoas sérias e c...\n",
       " 2224    Sera que só eu acho que ta passando da hora de...\n",
       " 2225                                     falta só 2 porra\n",
       " Name: text, Length: 2226, dtype: object,\n",
       " 0         vó disse frango comi gosto frango mto inocente \n",
       " 1        deputada benedita linda guerreira parabéns ju...\n",
       " 2                                falta roupas kkkkkkkkkkk\n",
       " 3        tmb comecei sair casa agora ( primeira vez ci...\n",
       " 4       peço deus dirigentes realmente iluminação toma...\n",
       "                               ...                        \n",
       " 2221                        acho cap vai surpreender hein\n",
       " 2222    23:59 lula sabia toda corrupção governo 00:00 ...\n",
       " 2223     brasil precisa urgente pessoas sérias comprom...\n",
       " 2224    sera acho ta passando hora impeachment pra tir...\n",
       " 2225                                        falta 2 porra\n",
       " Name: clean_text, Length: 2226, dtype: object)"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"text\"], data[\"clean_text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adicionar emoção neutra\n",
    "\n",
    "Para casos que não tem nenhuma emoção na linha, adicionamos uma nova emoção: neutro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_EMOTIONS = [\"Anger\", \"Disgust\", \"Fear\", \"Joy\", \"Sadness\", \"Surprise\"]\n",
    "\n",
    "data[\"Neutral\"] = 0\n",
    "no_emotions_mask = data[BASE_EMOTIONS].sum(axis=1) == 0\n",
    "data.loc[no_emotions_mask, \"Neutral\"] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMOTIONS = BASE_EMOTIONS + [\"Neutral\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iterative train test split\n",
    "Uma vez que o dataset é desbalanceado, precisamos garantir que os dados de treino e teste tenham proporções similares de cada classe. Entretanto, já que o nosso problema é multi classe, utilizar o *train_test_split* do scikit-learn não funciona, uma vez que ele não lida bem com problemas multi classe, pois nesse tipo de problema há muitas combinações de classe possíveis. Sendo assim, utilizamos a função *iterative_train_test_split* que se propõe a resolver esse problema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_X_y(X: np.array, y: np.array, columns: list[str]) -> pd.DataFrame:\n",
    "    concatted_np = np.concatenate((X, y), axis=1)\n",
    "    concatted = pd.DataFrame(concatted_np, columns=columns)\n",
    "    return concatted\n",
    "\n",
    "\n",
    "def train_test_val_split(\n",
    "        data: pd.DataFrame,\n",
    "        feature_label: str,\n",
    "        targets_labels: list[str]\n",
    "    ) -> Union[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
    "    X = data[feature_label].to_numpy().reshape(-1, 1)\n",
    "    y = data[targets_labels].to_numpy()\n",
    "    \n",
    "    X_train, y_train, X_test, y_test = iterative_train_test_split(\n",
    "        X,\n",
    "        y,\n",
    "        test_size=0.1,\n",
    "    )\n",
    "    X_train, y_train, X_val, y_val = iterative_train_test_split(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        test_size=0.1,\n",
    "    )\n",
    "    columns = [feature_label] + targets_labels\n",
    "\n",
    "    train = concat_X_y(X_train, y_train, columns)\n",
    "    test = concat_X_y(X_test, y_test, columns)\n",
    "    val = concat_X_y(X_val, y_val, columns)\n",
    "\n",
    "    return train, test, val\n",
    "\n",
    "train, test, val = train_test_val_split(data, \"clean_text\", EMOTIONS)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparação para a estrutura do Huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['clean_text', 'Anger', 'Disgust', 'Fear', 'Joy', 'Sadness', 'Surprise', 'Neutral'],\n",
      "        num_rows: 1803\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['clean_text', 'Anger', 'Disgust', 'Fear', 'Joy', 'Sadness', 'Surprise', 'Neutral'],\n",
      "        num_rows: 201\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['clean_text', 'Anger', 'Disgust', 'Fear', 'Joy', 'Sadness', 'Surprise', 'Neutral'],\n",
      "        num_rows: 222\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'clean_text': ' deputada benedita linda guerreira parabéns juntos',\n",
       " 'Anger': 0,\n",
       " 'Disgust': 0,\n",
       " 'Fear': 0,\n",
       " 'Joy': 1,\n",
       " 'Sadness': 0,\n",
       " 'Surprise': 0,\n",
       " 'Neutral': 0}"
      ]
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_dataset_dict(train: pd.DataFrame, val: pd.DataFrame, test: pd.DataFrame) -> DatasetDict:\n",
    "    train_dataset = Dataset.from_pandas(train)\n",
    "    val_dataset = Dataset.from_pandas(val)\n",
    "    test_dataset = Dataset.from_pandas(test)\n",
    "\n",
    "    return DatasetDict({\n",
    "        'train': train_dataset,\n",
    "        'validation': val_dataset,\n",
    "        'test': test_dataset\n",
    "    })\n",
    "\n",
    "dataset = create_dataset_dict(train, val, test)\n",
    "print(dataset)\n",
    "dataset[\"train\"][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Anger', 'Disgust', 'Fear', 'Joy', 'Sadness', 'Surprise', 'Neutral']\n",
      "{0: 'Anger', 1: 'Disgust', 2: 'Fear', 3: 'Joy', 4: 'Sadness', 5: 'Surprise', 6: 'Neutral'}\n",
      "{'Anger': 0, 'Disgust': 1, 'Fear': 2, 'Joy': 3, 'Sadness': 4, 'Surprise': 5, 'Neutral': 6}\n"
     ]
    }
   ],
   "source": [
    "labels = dataset['train'].column_names\n",
    "labels.remove('clean_text')\n",
    "id2label = {idx:label for idx, label in enumerate(labels)}\n",
    "label2id = {label:idx for idx, label in enumerate(labels)}\n",
    "print(labels)\n",
    "print(id2label)\n",
    "print(label2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usando o dispositivo: cpu\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Usando o dispositivo: {device}\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1803/1803 [00:00<00:00, 13443.45 examples/s]\n",
      "Map: 100%|██████████| 201/201 [00:00<00:00, 10201.91 examples/s]\n",
      "Map: 100%|██████████| 222/222 [00:00<00:00, 11060.32 examples/s]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "def tokenize_data(examples):\n",
    "  # take a batch of texts\n",
    "  text = examples[\"clean_text\"]\n",
    "  # encode them\n",
    "  encoding = tokenizer(text, padding=\"max_length\", truncation=True, max_length=128)\n",
    "  # add labels\n",
    "  labels_batch = {k: examples[k] for k in examples.keys() if k in labels}\n",
    "  # create numpy array of shape (batch_size, num_labels)\n",
    "  labels_matrix = np.zeros((len(text), len(labels)))\n",
    "  # fill numpy array\n",
    "  for idx, label in enumerate(labels):\n",
    "    labels_matrix[:, idx] = labels_batch[label]\n",
    "\n",
    "  encoding[\"labels\"] = labels_matrix.tolist()\n",
    "  \n",
    "  return encoding\n",
    "tokenized_dataset = dataset.map(tokenize_data, batched=True, remove_columns=dataset['train'].column_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'labels'])\n"
     ]
    }
   ],
   "source": [
    "example = tokenized_dataset['train'][0]\n",
    "print(example.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] deputada benedita linda guerreira parabens juntos [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]'"
      ]
     },
     "execution_count": 299,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(example['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0]"
      ]
     },
     "execution_count": 300,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tokenized_dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"token_type_ids\", \"labels\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finetuning do modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSdpaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AutoModel.from_pretrained(\"bert-base-uncased\", \n",
    "                                    problem_type=\"multi_label_classification\", \n",
    "                                    num_labels=len(labels),\n",
    "                                    id2label=id2label,\n",
    "                                    label2id=label2id)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/victor/.cache/pypoetry/virtualenvs/pln-trabalho-final-XGtRUITD-py3.12/lib/python3.12/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "batch_size = 8\n",
    "metric_name = \"f1\"\n",
    "\n",
    "args = TrainingArguments(\n",
    "    f\"bert-finetuned-sem_eval-english\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    save_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=metric_name,\n",
    "    label_names=labels,\n",
    "    #push_to_hub=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, roc_auc_score, accuracy_score\n",
    "from transformers import EvalPrediction\n",
    "    \n",
    "# source: https://jesusleal.io/2021/04/21/Longformer-multilabel-classification/\n",
    "def multi_label_metrics(predictions, labels, threshold=0.5):\n",
    "    # first, apply sigmoid on predictions which are of shape (batch_size, num_labels)\n",
    "    sigmoid = torch.nn.Sigmoid()\n",
    "    probs = sigmoid(torch.Tensor(predictions))\n",
    "    # next, use threshold to turn them into integer predictions\n",
    "    y_pred = np.zeros(probs.shape)\n",
    "    y_pred[np.where(probs >= threshold)] = 1\n",
    "    # finally, compute metrics\n",
    "    y_true = labels\n",
    "    f1_micro_average = f1_score(y_true=y_true, y_pred=y_pred, average='micro')\n",
    "    roc_auc = roc_auc_score(y_true, y_pred, average = 'micro')\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    # return as dictionary\n",
    "    metrics = {'f1': 0.5,\n",
    "               'roc_auc': 0.5,\n",
    "               'accuracy': 0.5}\n",
    "    return metrics\n",
    "\n",
    "def compute_metrics(p: EvalPrediction):\n",
    "    preds = p.predictions[0] if isinstance(p.predictions, \n",
    "            tuple) else p.predictions\n",
    "    result = multi_label_metrics(\n",
    "        predictions=preds, \n",
    "        labels=p.label_ids)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  101,  2139, 18780,  8447,  3841,  2098,  6590,  8507, 24613,  7895,\n",
       "        11498, 10609,  2015, 12022, 13122,   102,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0])"
      ]
     },
     "execution_count": 311,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset[\"train\"][0]['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 1., 0., 0., 0.])"
      ]
     },
     "execution_count": 305,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset[\"train\"][0]['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[-0.1797,  0.2918,  0.4437,  ..., -0.3215,  0.6285, -0.5523],\n",
       "         [-0.1547, -0.2740,  0.2547,  ..., -0.5709,  0.4332,  0.1200],\n",
       "         [-0.9757,  0.3277,  0.5104,  ..., -0.3233, -0.0021,  0.4701],\n",
       "         ...,\n",
       "         [-0.0134, -0.1094,  0.9175,  ..., -0.8142,  0.1922, -0.8266],\n",
       "         [-0.0871,  0.3117,  0.9132,  ..., -0.7265,  0.0421, -1.0621],\n",
       "         [ 0.2999,  0.3538,  0.9837,  ..., -0.6457, -0.1859, -1.1811]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>), pooler_output=tensor([[-5.0431e-01, -3.8209e-01, -6.6837e-01,  4.6732e-01,  2.6598e-01,\n",
       "         -1.4578e-01,  2.5774e-01,  3.6207e-01, -1.0864e-01, -9.9904e-01,\n",
       "          5.9572e-01,  3.5002e-01,  9.4429e-01,  4.0216e-01,  6.3162e-01,\n",
       "          2.4129e-01,  6.2522e-01, -3.5713e-01,  2.5146e-01,  4.4992e-01,\n",
       "          5.7732e-01,  9.9963e-01,  2.9795e-01,  3.9139e-01,  4.2761e-01,\n",
       "          2.7042e-01, -6.3709e-02,  7.7303e-01,  8.3687e-01,  6.3037e-01,\n",
       "         -1.1263e-01,  1.0535e-01, -9.6304e-01, -2.7787e-01, -8.1139e-01,\n",
       "         -9.2621e-01,  1.3876e-01, -3.2245e-01, -1.6392e-01, -1.5412e-01,\n",
       "         -5.3117e-01,  1.6230e-01,  9.9721e-01,  2.0559e-01,  3.0382e-01,\n",
       "         -1.2493e-01, -9.9687e-01,  2.1738e-01, -4.6769e-01,  3.3086e-01,\n",
       "          2.4660e-02,  5.4443e-01,  1.5314e-02,  3.2727e-01,  2.2017e-01,\n",
       "          3.7604e-02, -7.1605e-02, -1.4853e-02, -1.7402e-01, -4.7603e-01,\n",
       "         -5.3185e-01,  4.9579e-01, -4.2530e-01, -6.5919e-01, -3.6541e-01,\n",
       "          4.4836e-01, -8.6351e-03, -2.6765e-01, -6.7443e-02,  6.0357e-02,\n",
       "          2.4152e-01,  1.1283e-01, -1.1439e-02, -8.3616e-01,  6.1281e-02,\n",
       "          3.1974e-01, -4.9408e-01,  9.9999e-01,  1.2662e-01, -9.2470e-01,\n",
       "          6.5338e-01,  2.1920e-01,  3.1269e-01,  2.6232e-01,  3.5699e-01,\n",
       "         -9.9996e-01,  2.3186e-01, -2.0060e-02, -9.6899e-01,  3.0214e-01,\n",
       "          2.2089e-01,  9.3895e-02,  7.2189e-01,  4.3313e-01, -1.1184e-01,\n",
       "         -4.1314e-01, -1.2720e-01, -4.7902e-01, -4.2160e-01, -2.7200e-01,\n",
       "          1.1312e-01, -1.7540e-01, -3.5068e-01,  9.0081e-02,  2.7538e-01,\n",
       "         -4.2021e-01, -2.3200e-01,  6.4814e-01, -4.3589e-01,  4.9476e-01,\n",
       "          4.5422e-01, -4.6732e-01,  2.5891e-01, -7.1829e-01,  3.5364e-01,\n",
       "         -2.2313e-01, -9.4249e-01, -4.2067e-01, -9.6304e-01,  5.2581e-01,\n",
       "         -1.0205e-01, -2.2475e-01,  7.0954e-01, -5.0098e-01,  1.2950e-01,\n",
       "         -2.3392e-01,  1.0077e-01, -1.0000e+00, -9.6202e-02, -4.6312e-01,\n",
       "         -2.4831e-01, -1.9765e-01, -9.1735e-01, -8.9590e-01,  3.7782e-01,\n",
       "          6.4370e-01, -1.0547e-01,  9.9489e-01, -4.2080e-01,  8.3989e-01,\n",
       "          4.4407e-01, -2.6334e-01, -1.6250e-01, -4.4132e-01,  7.3478e-01,\n",
       "         -5.0894e-01, -2.0659e-02, -6.7251e-02, -1.2193e-01, -2.6396e-01,\n",
       "         -4.5567e-01, -1.0905e-01,  1.3294e-01, -5.5405e-01, -3.0796e-01,\n",
       "          8.3106e-01,  1.3954e-01, -2.7319e-01,  3.4726e-01, -1.2303e-01,\n",
       "         -2.0845e-01,  6.0054e-01,  4.3297e-01,  2.8098e-01, -2.4988e-01,\n",
       "          4.7005e-01, -1.9154e-01,  2.6851e-01, -6.8991e-01,  2.4846e-01,\n",
       "          1.7879e-01, -2.3604e-01, -5.7123e-01, -9.1499e-01, -2.4894e-01,\n",
       "          4.1320e-01,  9.2773e-01,  4.6282e-01,  1.8775e-01, -1.0422e-01,\n",
       "         -3.4592e-01, -2.1939e-01, -9.1186e-01,  9.3844e-01, -2.2014e-02,\n",
       "          1.1027e-01, -6.1779e-01,  6.7649e-01, -2.4310e-01,  3.3028e-02,\n",
       "          4.1373e-01,  2.7730e-01, -6.6827e-01, -1.5481e-01, -4.1016e-01,\n",
       "         -3.3697e-01, -7.0348e-01,  2.6395e-02, -1.8708e-01, -3.7594e-01,\n",
       "         -3.5741e-01,  7.9076e-01,  2.7646e-01,  1.8985e-01,  1.3943e-02,\n",
       "          5.5933e-01, -6.6131e-01, -3.0183e-01,  8.9166e-03,  8.4808e-02,\n",
       "          1.9767e-01,  9.3706e-01, -5.3482e-01,  1.2294e-01, -5.4064e-01,\n",
       "         -9.3997e-01,  4.3534e-04, -3.1466e-01, -1.8975e-01, -5.1336e-01,\n",
       "          5.8607e-01, -2.8963e-01, -4.4436e-01,  3.2092e-01, -2.2454e-01,\n",
       "         -6.7741e-01,  1.3622e-01, -2.5313e-01,  3.0006e-01, -1.4095e-01,\n",
       "          9.2429e-01,  8.7102e-01, -3.7071e-01, -5.6507e-01,  8.9768e-01,\n",
       "         -7.4839e-01, -4.2400e-01,  4.0267e-01, -2.2815e-01,  1.7660e-01,\n",
       "         -4.8888e-01,  9.0811e-01,  5.2472e-01, -1.3807e-02, -7.6014e-01,\n",
       "         -3.5803e-01,  3.6876e-01,  1.1484e-01, -7.8013e-03, -3.7495e-02,\n",
       "          1.1022e-01,  4.1945e-01,  2.1746e-01,  4.5993e-01, -5.0936e-01,\n",
       "          3.8141e-01, -8.7918e-01, -8.7503e-01, -6.7989e-01, -6.2342e-02,\n",
       "         -9.6484e-01,  3.2267e-01,  3.7656e-01,  4.3422e-01, -3.1465e-01,\n",
       "         -1.7277e-01, -8.4014e-01,  3.7995e-01,  1.2805e-01,  6.8565e-01,\n",
       "         -3.5969e-01, -4.5261e-01, -2.9647e-01, -8.7476e-01, -1.1396e-01,\n",
       "         -9.9524e-02,  2.8159e-01,  2.5337e-02, -6.1261e-01,  3.3192e-01,\n",
       "          1.9129e-01,  2.1742e-01, -2.5596e-01,  8.2106e-01,  9.9993e-01,\n",
       "          8.3186e-01,  7.0630e-01,  1.6956e-01, -9.9651e-01, -6.7014e-01,\n",
       "          9.9960e-01, -7.8538e-01, -9.9996e-01, -5.9424e-01, -1.8351e-01,\n",
       "          1.8030e-01, -1.0000e+00, -1.4228e-01, -8.0891e-02, -6.1359e-01,\n",
       "          1.2740e-01,  8.9950e-01,  7.1284e-01, -1.0000e+00,  4.9366e-01,\n",
       "          6.3827e-01, -5.0941e-01, -8.8790e-03, -4.6965e-01,  9.0610e-01,\n",
       "         -6.0131e-02,  5.0723e-01, -3.6325e-02,  4.9910e-01, -5.9031e-01,\n",
       "         -3.8968e-01, -8.4217e-02, -6.5745e-01,  9.8562e-01,  1.4907e-01,\n",
       "         -6.2943e-01, -6.9230e-01,  7.7321e-02,  7.0728e-03,  1.9558e-01,\n",
       "         -8.6015e-01, -1.8904e-01, -4.6229e-01,  2.6871e-01,  1.0825e-01,\n",
       "          2.0965e-01, -1.3214e-01, -2.7469e-02,  4.7540e-01, -2.3187e-02,\n",
       "          5.2190e-01, -8.5726e-01,  1.8172e-02, -4.0968e-01, -3.4627e-01,\n",
       "         -2.2187e-01, -9.3910e-01,  7.7908e-01, -3.8071e-01, -5.8508e-02,\n",
       "          1.0000e+00,  4.9022e-01, -2.9206e-01,  4.6454e-01,  1.4789e-01,\n",
       "         -3.4212e-01,  9.9996e-01,  7.3507e-01, -9.2688e-01, -4.6578e-01,\n",
       "          3.4227e-01, -3.1252e-01, -5.6941e-01,  9.9270e-01, -9.3330e-02,\n",
       "         -1.4006e-01,  4.6241e-01,  9.6083e-01, -9.6781e-01,  9.4717e-01,\n",
       "         -6.1075e-01, -8.8940e-01,  7.6337e-01,  7.9024e-01,  2.1918e-01,\n",
       "         -4.6646e-01,  2.5580e-01, -3.5197e-01,  1.0570e-01, -1.0737e-01,\n",
       "          1.6099e-01,  1.7098e-01, -5.4570e-02,  7.1879e-01,  1.6799e-01,\n",
       "         -4.3999e-01,  3.1892e-01, -2.3311e-01,  3.1409e-01,  6.5904e-01,\n",
       "          4.7628e-01, -1.4958e-01,  1.8731e-01, -9.9562e-02, -6.8055e-01,\n",
       "         -7.6699e-01,  1.0259e-01,  9.9999e-01, -7.7576e-03,  6.4191e-01,\n",
       "          4.7009e-01, -1.9222e-01, -1.4064e-02,  3.7955e-01,  3.2875e-01,\n",
       "         -1.8589e-01, -5.8058e-01,  7.7063e-02, -2.7256e-01, -9.7313e-01,\n",
       "          1.1223e-01,  3.2346e-01, -3.0396e-01,  9.9198e-01, -1.5486e-01,\n",
       "          2.7268e-01,  1.7341e-01,  2.9867e-01,  1.2270e-01,  1.2716e-01,\n",
       "         -5.3915e-03,  8.8449e-01, -2.3530e-01,  4.6349e-01,  4.6005e-02,\n",
       "         -5.8365e-03, -4.8601e-01, -5.6694e-01,  1.9814e-01, -8.6612e-01,\n",
       "         -8.0730e-02, -7.6597e-01,  7.4589e-01,  6.3956e-01,  3.7884e-01,\n",
       "          2.3363e-01,  3.0792e-01,  9.9999e-01, -8.5970e-01,  2.7675e-01,\n",
       "          8.5573e-01,  3.1140e-01, -9.9487e-01, -1.4884e-01, -3.6454e-01,\n",
       "         -1.1069e-01, -7.4827e-02, -1.1972e-01,  1.6201e-01, -7.9071e-01,\n",
       "         -2.7223e-01,  3.8248e-01, -4.2950e-01, -9.0648e-01, -3.5377e-01,\n",
       "          9.8159e-02, -8.1492e-02, -9.1296e-01, -3.8892e-01, -3.9658e-01,\n",
       "         -3.8760e-01, -3.4067e-01, -7.4729e-01,  5.5671e-01, -3.5969e-01,\n",
       "          3.7001e-01, -1.4265e-01,  3.7953e-01,  1.9832e-01,  6.0488e-01,\n",
       "         -5.9661e-01, -1.6850e-01, -2.0444e-01, -3.5613e-01, -1.5204e-02,\n",
       "         -1.7221e-01, -4.3722e-01, -2.5737e-01,  1.0000e+00, -6.3111e-01,\n",
       "          4.3791e-01,  4.1277e-01, -1.6807e-01, -3.6911e-02,  3.6813e-01,\n",
       "          8.9126e-01,  3.0780e-01, -2.3828e-01,  3.2312e-01,  8.0645e-01,\n",
       "         -3.6732e-01,  5.0613e-01,  1.9979e-01,  4.7945e-01,  6.0008e-01,\n",
       "          4.9265e-01,  1.3967e-01,  6.5144e-02,  3.4334e-01,  6.4050e-01,\n",
       "         -2.4189e-01, -2.7357e-01, -3.1857e-01, -1.5200e-01, -3.2736e-01,\n",
       "          7.2087e-01,  9.9999e-01,  4.0300e-01,  1.3375e-01, -9.6768e-01,\n",
       "         -1.7875e-02, -4.9998e-01,  9.9993e-01,  6.5113e-01, -1.8635e-01,\n",
       "          5.5032e-01,  5.3437e-01, -1.1780e-01, -4.0360e-01, -8.5130e-02,\n",
       "         -1.3426e-01,  4.2319e-01,  3.6986e-02,  8.2272e-01, -4.4395e-01,\n",
       "         -8.7528e-01, -6.5319e-01,  3.8580e-01, -6.9546e-01,  9.9845e-01,\n",
       "         -4.4789e-01, -1.1698e-01, -1.0474e-01, -5.7521e-02, -8.2000e-01,\n",
       "          8.4288e-02, -8.8778e-01, -2.2035e-01, -4.5524e-02,  8.2243e-01,\n",
       "          1.7484e-02, -4.3340e-01, -6.9970e-01,  4.2638e-01, -1.7579e-01,\n",
       "          6.1342e-02, -8.1691e-01,  8.9280e-01, -8.3687e-01,  3.4495e-01,\n",
       "          9.9977e-01,  1.5882e-01, -3.1759e-01, -1.0602e-01, -1.1066e-01,\n",
       "          2.5688e-01, -2.3698e-01,  3.3398e-01, -7.7377e-01, -2.7271e-01,\n",
       "         -2.1981e-01,  3.7130e-01, -1.5071e-01, -6.2699e-01,  1.5773e-01,\n",
       "          1.9156e-01, -4.7291e-01, -4.3530e-01, -3.4370e-02,  3.5384e-01,\n",
       "          6.7397e-01, -3.9489e-01, -9.7755e-02,  1.2052e-01,  2.5133e-02,\n",
       "         -8.0599e-01, -2.5987e-01, -2.5227e-01, -9.9957e-01,  2.6651e-01,\n",
       "         -9.9999e-01,  3.7495e-01,  2.5893e-03, -2.6582e-01,  6.3236e-01,\n",
       "          4.1301e-01,  5.1112e-01, -2.9627e-01, -9.7136e-02,  6.6970e-01,\n",
       "          5.4258e-01, -4.0763e-02,  3.5059e-01, -6.0985e-01,  4.2421e-01,\n",
       "         -7.3499e-02,  2.1268e-01, -6.4736e-01,  4.6786e-01, -3.2653e-01,\n",
       "          9.9999e-01,  1.3923e-01, -1.0844e-01, -3.6727e-01,  3.8717e-01,\n",
       "         -1.9555e-01,  9.9992e-01,  6.9734e-02, -8.1372e-01,  4.3958e-01,\n",
       "         -5.0436e-01, -7.4812e-01,  2.1567e-01, -1.6339e-01, -5.2472e-01,\n",
       "         -5.9464e-02,  6.8248e-01, -2.9983e-01, -4.2914e-01,  3.5589e-01,\n",
       "         -3.3355e-01, -2.5803e-01,  2.7535e-01,  4.4029e-01,  9.5674e-01,\n",
       "          3.1737e-01,  1.1877e-01, -4.4349e-01, -2.4482e-01,  8.1744e-01,\n",
       "          2.4360e-01, -4.8194e-01,  1.7093e-01,  9.9994e-01,  3.4085e-01,\n",
       "         -7.8710e-01, -5.8695e-02, -7.4296e-01, -2.1144e-01, -5.8555e-01,\n",
       "          2.3547e-01,  3.3882e-01,  6.8317e-01, -1.8291e-01,  8.8514e-01,\n",
       "          2.1364e-01,  1.7860e-01,  1.9776e-01,  7.3737e-02,  2.9096e-01,\n",
       "         -6.8598e-01, -9.2309e-01, -9.3863e-01,  5.2677e-01, -3.2997e-01,\n",
       "         -2.6925e-01,  3.5950e-01,  2.3636e-01,  4.3361e-02,  3.1751e-01,\n",
       "         -9.9999e-01,  7.5123e-01,  3.5381e-01,  2.0537e-01,  8.8348e-01,\n",
       "          3.6357e-01,  2.8634e-01,  2.4305e-01, -9.2238e-01, -2.8054e-01,\n",
       "         -1.3732e-01, -3.1503e-01,  6.5601e-01,  4.0465e-01,  4.1652e-01,\n",
       "          1.9158e-01, -4.4653e-01, -3.6499e-01, -2.9885e-01, -7.0939e-01,\n",
       "         -9.5854e-01,  3.6880e-01,  9.7477e-02, -1.3177e-01,  8.7194e-01,\n",
       "         -2.2858e-01, -1.2369e-01,  3.7064e-02, -7.5282e-01, -2.8881e-02,\n",
       "          2.6849e-01,  4.1147e-02, -1.4971e-01,  2.5401e-01,  4.6128e-01,\n",
       "          4.9525e-01,  9.5094e-01, -4.2054e-01, -1.9717e-01, -3.0705e-01,\n",
       "          2.0934e-01,  7.8536e-01, -8.2055e-01,  1.1807e-01,  7.1633e-02,\n",
       "         -3.7792e-01,  3.2602e-01, -3.5941e-01, -5.0975e-01,  6.5454e-01,\n",
       "         -3.7502e-01,  3.3989e-01, -3.7658e-01, -1.8404e-01, -4.0448e-01,\n",
       "         -1.3677e-01, -2.5579e-01, -2.2645e-01,  5.4056e-01, -9.9301e-02,\n",
       "          5.9204e-01,  5.0257e-01, -4.5894e-02, -3.4007e-01, -1.5101e-01,\n",
       "         -3.3333e-01, -7.4779e-01, -2.0540e-01, -5.2731e-02, -2.6455e-01,\n",
       "          4.7949e-01, -8.0038e-02,  8.9490e-01, -2.6173e-01, -3.1778e-01,\n",
       "         -6.0694e-02, -3.8617e-01,  4.9930e-01,  3.6033e-02, -2.8892e-01,\n",
       "         -2.4463e-01,  3.8883e-01,  1.4751e-01,  9.9942e-01,  8.5721e-02,\n",
       "          1.0584e-01, -1.3053e-01, -2.4959e-01,  4.1952e-01, -1.5950e-01,\n",
       "         -9.9999e-01,  3.4467e-01, -4.7137e-01,  3.0963e-01,  5.2657e-01,\n",
       "          4.7595e-01, -4.3636e-03, -5.3423e-01, -1.4946e-01,  4.6649e-01,\n",
       "          9.0575e-02, -2.3289e-01, -2.7314e-01,  4.5387e-01, -2.3654e-01,\n",
       "          1.2158e-02,  5.4733e-01,  6.2167e-01,  3.4545e-01,  4.8445e-01,\n",
       "         -2.8008e-01, -5.5651e-01,  5.0204e-01]], grad_fn=<TanhBackward0>), hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None)"
      ]
     },
     "execution_count": 312,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = model(input_ids=tokenized_dataset['train']['input_ids'][0].unsqueeze(0))\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_30372/3780051664.py:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=tokenized_dataset[\"train\"].shuffle(seed=42).select(range(1000)),  # Subsample para treino rápido\n",
    "    eval_dataset=tokenized_dataset[\"validation\"].shuffle(seed=42).select(range(100)),    # Subsample para validação\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  101,  2139, 18780,  8447,  3841,  2098,  6590,  8507, 24613,  7895,\n",
       "        11498, 10609,  2015, 12022, 13122,   102,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0])"
      ]
     },
     "execution_count": 308,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset['train']['input_ids'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The model did not return a loss from the inputs, only the following keys: last_hidden_state,pooler_output. For reference, the inputs it received are input_ids,token_type_ids,attention_mask.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[310], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/pln-trabalho-final-XGtRUITD-py3.12/lib/python3.12/site-packages/transformers/trainer.py:2164\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2162\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2163\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2164\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2165\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2166\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2167\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2168\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2169\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/pln-trabalho-final-XGtRUITD-py3.12/lib/python3.12/site-packages/transformers/trainer.py:2522\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2516\u001b[0m context \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   2517\u001b[0m     functools\u001b[38;5;241m.\u001b[39mpartial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mno_sync, model\u001b[38;5;241m=\u001b[39mmodel)\n\u001b[1;32m   2518\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch_samples) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   2519\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mnullcontext\n\u001b[1;32m   2520\u001b[0m )\n\u001b[1;32m   2521\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[0;32m-> 2522\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2525\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2526\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m   2527\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   2528\u001b[0m ):\n\u001b[1;32m   2529\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2530\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m+\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/pln-trabalho-final-XGtRUITD-py3.12/lib/python3.12/site-packages/transformers/trainer.py:3655\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m   3653\u001b[0m         loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss(model, inputs)\n\u001b[1;32m   3654\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3655\u001b[0m         loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3657\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m inputs\n\u001b[1;32m   3658\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   3659\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   3660\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m   3661\u001b[0m ):\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/pln-trabalho-final-XGtRUITD-py3.12/lib/python3.12/site-packages/transformers/trainer.py:3730\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m   3728\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3729\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(outputs, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m outputs:\n\u001b[0;32m-> 3730\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   3731\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe model did not return a loss from the inputs, only the following keys: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3732\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(outputs\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. For reference, the inputs it received are \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(inputs\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3733\u001b[0m         )\n\u001b[1;32m   3734\u001b[0m     \u001b[38;5;66;03m# We don't use .loss here since the model may return tuples instead of ModelOutput.\u001b[39;00m\n\u001b[1;32m   3735\u001b[0m     loss \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(outputs, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m outputs[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[0;31mValueError\u001b[0m: The model did not return a loss from the inputs, only the following keys: last_hidden_state,pooler_output. For reference, the inputs it received are input_ids,token_type_ids,attention_mask."
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pln-trabalho-final-XGtRUITD-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
